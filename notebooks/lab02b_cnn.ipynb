{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02b: Training a CNN on Synthetic Handwriting Data\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "- Fundamental principles for building neural networks with convolutional components\n",
    "- How to use Lightning's training framework via a CLI\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amazingguni/git/fsdl-text-recognizer-2022-labs\n",
      "Makefile   \u001b[0m\u001b[01;34mdata\u001b[0m/            \u001b[01;34mlightning_logs\u001b[0m/  \u001b[01;34mrequirements\u001b[0m/     \u001b[01;34mtraining\u001b[0m/\n",
      "README.md  environment.yml  \u001b[01;34mnotebooks\u001b[0m/       \u001b[01;34mtext_recognizer\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    # bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/emnist/U.png\" width=\"177\" height=\"177\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import IPython.display as display\n",
    "\n",
    "randsize = 10 ** (random.random() * 2 + 1)\n",
    "\n",
    "Url = \"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/emnist/U.png\"\n",
    "\n",
    "# run multiple times to display the same image at different sizes\n",
    "#  the content of the image remains unambiguous\n",
    "display.Image(url=Url, width=randsize, height=randsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions are the local, translation-equivariant linear transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic:\n",
      "tensor([[ 0.1797],\n",
      "        [ 0.5482],\n",
      "        [ 1.1588],\n",
      "        [-0.4887],\n",
      "        [-0.0285],\n",
      "        [-0.4871],\n",
      "        [ 0.7276],\n",
      "        [ 0.4638]])\n",
      "local:\n",
      "tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.3999],\n",
      "        [0.4685],\n",
      "        [0.0765],\n",
      "        [0.0000],\n",
      "        [0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "generic_linear_transform = torch.randn(8, 1)\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "local_linear_transform = torch.tensor([\n",
    "    [0, 0, 0] + [random.random(), random.random(), random.random()] + [0, 0]]).T\n",
    "print(\"local:\", local_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic:\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7]])\n",
      "translation invariant:\n",
      "tensor([[0, 7, 6, 5, 4, 3, 2, 1],\n",
      "        [1, 0, 7, 6, 5, 4, 3, 2],\n",
      "        [2, 1, 0, 7, 6, 5, 4, 3],\n",
      "        [3, 2, 1, 0, 7, 6, 5, 4],\n",
      "        [4, 3, 2, 1, 0, 7, 6, 5],\n",
      "        [5, 4, 3, 2, 1, 0, 7, 6],\n",
      "        [6, 5, 4, 3, 2, 1, 0, 7],\n",
      "        [7, 6, 5, 4, 3, 2, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "generic_linear_transform = torch.arange(8)[:, None]\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "equivariant_linear_transform = torch.stack([torch.roll(generic_linear_transform[:, 0], ii) for ii in range(8)], dim=1)\n",
    "print(\"translation invariant:\", equivariant_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.2425,  0.5438, -0.2380]]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the equivalent of torch.nn.Linear, but for a 1-dimensional convolution\n",
    "conv_layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "\n",
    "conv_layer.weight  # aka kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolution matrix:\n",
      "tensor([[ 0.2425,  0.5438, -0.2380,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.2425,  0.5438, -0.2380,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.2425,  0.5438, -0.2380,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.2425,  0.5438, -0.2380,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2425,  0.5438, -0.2380,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2425,  0.5438, -0.2380],\n",
      "        [-0.2380,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2425,  0.5438],\n",
      "        [ 0.5438, -0.2380,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2425]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_kernel_as_vector = torch.hstack([conv_layer.weight[0][0], torch.zeros(5)])\n",
    "conv_layer_as_matrix = torch.stack([torch.roll(conv_kernel_as_vector, ii) for ii in range(8)], dim=0)\n",
    "print(\"convolution matrix:\", conv_layer_as_matrix, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/56/RGB_channels_separation.png?20110219015028\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.Image(\n",
    "    url=\"https://upload.wikimedia.org/wikipedia/commons/5/56/RGB_channels_separation.png?20110219015028\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://distill.pub/2018/building-blocks/examples/input_images/dog_cat.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sample image\n",
    "display.Image(url=\"https://distill.pub/2018/building-blocks/examples/input_images/dog_cat.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/distill-feature-attrib.png\" width=\"1024\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://distill.pub/2018/building-blocks/\n",
    "display.Image(url=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/distill-feature-attrib.png\", width=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1024\"\n",
       "            height=\"720\"\n",
       "            src=\"https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d1_52.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        style=\"background: #FFF\";\n",
       "></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f452a44c880>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [\"conv2d0\", \"conv2d1\", \"conv2d2\", \"mixed3a\", \"mixed3b\"]\n",
    "layer = layers[1]\n",
    "idx = 52\n",
    "\n",
    "weight_explorer = display.IFrame(\n",
    "    src=f\"https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/{layer}_{idx}.html\", width=1024, height=720)\n",
    "weight_explorer.iframe = 'style=\"background: #FFF\";\\n><'.join(weight_explorer.iframe.split(\"><\"))  # inject background color\n",
    "weight_explorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying convolutions to handwritten characters: `CNN`s on `EMNIST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtext_recognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Simple CNN for recognizing characters in a square image.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_config\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_dims\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0minput_height\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput_width\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"input height and width should be equal, but was {input_height}, {input_width}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mapping\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mconv_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv_dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONV_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfc_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fc_dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFC_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfc_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fc_dropout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFC_DROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Because our 3x3 convs have padding size 1, they leave the input size unchanged.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# The 2x2 max-pool divides the input size by 2.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mconv_output_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_output_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_height\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_width\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_input_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_output_height\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconv_output_width\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconv_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_input_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Applies the CNN to x.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        x\u001b[0m\n",
      "\u001b[0;34m            (B, Ch, H, W) tensor, where H and W must equal input height and width from data_config.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        torch.Tensor\u001b[0m\n",
      "\u001b[0;34m            (B, Cl) tensor\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0m_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_height\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"bad inputs to CNN with shape {x.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, CONV_DIM, H, W\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, CONV_DIM, H, W\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, CONV_DIM, H // 2, W // 2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, CONV_DIM * H // 2 * W // 2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, FC_DIM\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, Cl\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0madd_to_argparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--conv_dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONV_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--fc_dim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFC_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--fc_dropout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFC_DROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           ~/git/fsdl-text-recognizer-2022-labs/text_recognizer/models/cnn.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "import text_recognizer.models\n",
    "\n",
    "\n",
    "text_recognizer.models.CNN??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `EMNIST` Handwritten Character Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNIST dataset of handwritten characters and digits.\n",
      "\n",
      "    \"The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19\n",
      "    and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset.\"\n",
      "    From https://www.nist.gov/itl/iad/image-group/emnist-dataset\n",
      "\n",
      "    The data split we will use is\n",
      "    EMNIST ByClass: 814,255 characters. 62 unbalanced classes.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import text_recognizer.data\n",
    "\n",
    "emnist = text_recognizer.data.EMNIST()\n",
    "print(emnist.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist.prepare_data()  # download, save to disk\n",
    "emnist.setup()  # create torch.utils.data.Datasets, do train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amazingguni/git/fsdl-text-recognizer-2022-labs/data\n",
      "downloaded  processed  raw\n",
      "metadata.toml  readme.md\n"
     ]
    }
   ],
   "source": [
    "!echo {emnist.data_dirname()}\n",
    "!ls {emnist.data_dirname()}\n",
    "!ls {emnist.data_dirname() / \"raw\" / \"emnist\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMNIST Dataset\n",
       "Num classes: 83\n",
       "Mapping: ['<B>', '<S>', '<E>', '<P>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '?']\n",
       "Dims: (1, 28, 28)\n",
       "Train/val/test sizes: 260212, 65054, 53988\n",
       "Batch x stats: (torch.Size([128, 1, 28, 28]), torch.float32, tensor(0.), tensor(0.1672), tensor(0.3262), tensor(1.))\n",
       "Batch y stats: (torch.Size([128]), torch.int64, tensor(4), tensor(65))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = next(iter(emnist.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACq0lEQVR4nNWVvU7rMBiGbec4DUrDz5aKJZGiSgihZqjEDcDCglhZuASuhGtg4iroWgFjVSEkiBqVKUgkJY5NiV07Z7BUiUNbWs5yzrvlc/zo1ffjD4D/RXDeAUIIIQQAUEoppf4WCiE0TdNxnM3NzclkQilN07SqquWhv76GTNNsNBp7e3utVosxFkXR9fV1WZYrcT/JMAzP887Pz29ubj4+PoqieHx8PD099X2/Vqv93KkWhBBjjDHe3t4+PDxkjDHG0jSVUn4LRX98V1VFKR0MBg8PD5xzCKFlWWEYhmHYaDSWNzvDo23b7XY7iiLOuVJqPB5HUXR5eXl8fGwYxspOtdmyLLMsy7JM18c0Tdd1wzBstVoY459AAQBKKULI/f39y8uLTkKtVtMtsb6+vozZ2YIQttvti4uLwWCg+18IkSTJ2dlZEASWZS24O7f6VVURQgghnHMdMQxjY2Pj4OBgMplIKeM4XhkKAOCcF0VRFMU0Yprm/v6+EIIQMhwO543vImiSJJ1Op16v7+zsrK2tQQghhJ7nIYTSNO10OuPxeObF2YXS0kns9XpJkkyTgBByHMf3fcdx5lVsEVRKmed5v9/v9/t5nutZ0tAgCFzXnddei6AAgLIsh8Ph1dVVt9vNskwn0bKsZrN5cnLSbDZntsE3UO03juM4jhlj08pgjHd3d4MgcBxHP7urQQEAlFLGmBACAAAhBAAYhuH7vud5tm3/EKqU0r0ppZy+qvV63bbtmWldCiqEKIpiNBqNRiMhRFVVUkpCCKVU2/8JlFL69PR0d3d3e3v79vb2/v7++vra6/WiKKKUfh2BuYvv008QYoxt297a2jo6OjIM4/n5udvt5nnOOf+6ZpaCaiGEMMau60IIGWNZls3bAitAtfQUVVW16t7+J/UbHCl6vQEdS50AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F4466C31880>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "idx = random.randint(0, len(xs) - 1)\n",
    "\n",
    "print(emnist.mapping[ys[idx]])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting convolutions in a `torch.nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=12544, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=83, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config = emnist.config()\n",
    "\n",
    "cnn = text_recognizer.models.CNN(data_config)\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAACrklEQVR4nNWVv0/qUBTH771tLVrSgjUpaCIssrhImEggLI7yN+h/4H/gYvgbDKvhP3AiwQEWJgbTRowSfkwgKU2LWPuTvuG+1PdefLQYFs/Yb+/nnnP6PacA/JSAATKEPM/HYjFN0xaLhW3bf0qxWIxhGIqiVFVVFMV1XSyRq6E8z1cqlWKx+Pj4OBqN5vO5LyGEjo+P0+k0wzCiKNZqtXq9bppmcCFHR0fdbne5XDqOY9u2ZVmWZZmmqeu6ruuGYZimiZ83Go2Dg4NQmSKECILwPE9RFL/8t7e3ZrPpZ82ybKFQuL+/n81mwVBcIMdxw+Hw+vpakiRN0wAAtm2/vr76/aUoShCE+XxuGEZw7SRJXl1d6bperVY5jkMIBZ/B2azQIIQEQUAIt7a2IISe520ASpJkNBoFAMiybBjGZqCCIJRKJdd1x+Ox4zghiQFQnud3d3en02mz2dwMNBKJnJ2dCYKApyU8EaywVCKRKJfLNE1rmpZMJkny95uqqqqq6rrucrlcG0rTNMuyBEHk8/larYZd6XmeJEmiKIqi2Gq1ZFleI3+api8uLqbTqSzLz8/P/X7//f3dNE3LsvBoDgaD8/NzgiDCZhqJRDKZzOXlJcdx1Wr15uZmZ2enVCpFo1EIIcuyxWJRkqRWq+WvpWBoIpE4PT3NZDKO4wyHw5eXF8dxHh4esIqHcrFY+JMeCsrz/P7+PkVR3W630WhgM/mWwjf9D4fjX0shhFKpVDqdBgD0er3JZLL6fCjo9vZ2Pp8/OTmxbbvdbq/r0K9uQKhcLvd6vY+Pj36/n8vlvsf5q6cIoWw2e3h4SJKkoigrPkVYKMMwruviFafr+t3d3fca+gmFEO7t7amqOpvNOp3O09PT7e1tqE3+VXz+oiGEAIB4PM6yLLZh+AX6Y+MXpipr3FSGfssAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F4516E16FD0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(xs) - 1)\n",
    "outs = cnn(xs[idx: idx + 1])\n",
    "print('output:', emnist.mapping[torch.argmax(outs)])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Applies the CNN to x.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        x\u001b[0m\n",
      "\u001b[0;34m            (B, Ch, H, W) tensor, where H and W must equal input height and width from data_config.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        torch.Tensor\u001b[0m\n",
      "\u001b[0;34m            (B, Cl) tensor\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0m_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_height\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"bad inputs to CNN with shape {x.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, CONV_DIM, H, W\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, CONV_DIM, H, W\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, CONV_DIM, H // 2, W // 2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, CONV_DIM * H // 2 * W // 2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, FC_DIM\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# _B, Cl\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/git/fsdl-text-recognizer-2022-labs/text_recognizer/models/cnn.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "cnn.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(torch.Tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[576, 64, 36864, 64, 1605632, 128, 10624, 83]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.numel() for p in cnn.parameters()]  # conv weight + bias, conv weight + bias, fc weight + bias, fc weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 12544]), 12544)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggest_layer = [p for p in cnn.parameters() if p.numel() == max(p.numel() for p in cnn.parameters())][0]\n",
    "biggest_layer.shape, cnn.fc_input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1605632"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the Linear layers, number of multiplications per input == nparams\n",
    "cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Conv2D layers, it's more complicated\n",
    "\n",
    "def approx_conv_multiplications(kernel_shape, input_size=(64, 28, 28)):  # this is a rough and dirty approximation\n",
    "    num_kernel_elements = 1\n",
    "    for dimension in kernel_shape[-3:]:\n",
    "        num_kernel_elements *= dimension\n",
    "    num_input_channels, num_kernels = input_size[0], kernel_shape[0]\n",
    "    num_spatial_applications = ((input_size[1] - kernel_shape[-2] + 1) * (input_size[2] - kernel_shape[-1] + 1))\n",
    "    mutliplications_per_kernel = num_spatial_applications * num_kernel_elements * num_input_channels\n",
    "    return mutliplications_per_kernel * num_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1594884096"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_conv_multiplications(cnn.conv2.conv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratio of multiplications in the convolution to multiplications in the fully-connected layer is huge!\n",
    "approx_conv_multiplications(cnn.conv2.conv.weight.shape) // cnn.fc1.weight.numel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a `CNN` on `EMNIST` with the Lightning `Trainer` and `run_experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        .. deprecated:: v1.5 Trainer argument\n",
      "                        ``terminate_on_nan`` was deprecated in v1.5 and will\n",
      "                        be removed in 1.7. Please use ``detect_anomaly``\n",
      "                        instead.\n",
      "\n",
      "Data Args:\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Number of examples to operate on per forward step.\n",
      "                        Default is 128.\n",
      "  --num_workers NUM_WORKERS\n",
      "                        Number of additional processes to load data. Default\n",
      "                        is 16.\n",
      "\n",
      "Model Args:\n",
      "  --conv_dim CONV_DIM\n",
      "  --fc_dim FC_DIM\n",
      "  --fc_dropout FC_DROPOUT\n",
      "\n",
      "LitModel Args:\n",
      "  --optimizer OPTIMIZER\n",
      "                        optimizer class from torch.optim\n",
      "  --lr LR\n",
      "  --one_cycle_max_lr ONE_CYCLE_MAX_LR\n",
      "  --one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS\n",
      "  --loss LOSS           loss function from torch.nn.functional\n"
     ]
    }
   ],
   "source": [
    "!python training/run_experiment.py --help --model_class CNN --data_class EMNIST  | tail -n 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Run an experiment.\n",
      "\n",
      "    Sample command:\n",
      "    ```\n",
      "    python training/run_experiment.py --max_epochs=3 --gpus='0,' --num_workers=20 --model_class=MLP --data_class=MNIST\n",
      "    ```\n",
      "\n",
      "    For basic help documentation, run the command\n",
      "    ```\n",
      "    python training/run_experiment.py --help\n",
      "    ```\n",
      "\n",
      "    The available command line args differ depending on some of the arguments, including --model_class and --data_class.\n",
      "\n",
      "    To see which command line args are available and read their documentation, provide values for those arguments\n",
      "    before invoking --help, like so:\n",
      "    ```\n",
      "    python training/run_experiment.py --model_class=MLP --data_class=MNIST --help\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import training.run_experiment\n",
    "\n",
    "\n",
    "print(training.run_experiment.main.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type      | Params\n",
      "---------------------------------------------\n",
      "0 | model          | CNN       | 1.7 M \n",
      "1 | model.conv1    | ConvBlock | 640   \n",
      "2 | model.conv2    | ConvBlock | 36.9 K\n",
      "3 | model.dropout  | Dropout   | 0     \n",
      "4 | model.max_pool | MaxPool2d | 0     \n",
      "5 | model.fc1      | Linear    | 1.6 M \n",
      "6 | model.fc2      | Linear    | 10.7 K\n",
      "7 | train_acc      | Accuracy  | 0     \n",
      "8 | val_acc        | Accuracy  | 0     \n",
      "9 | test_acc       | Accuracy  | 0     \n",
      "---------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.616     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model State Dict Disk Size: 6.62 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6391851b0b674af788f4a3b20a13d21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6e2eaebab54d29b5b1879622f8cc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f0a6b0d5a042338f0b0fccf31bcf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_3/epoch=0000-validation.loss=0.574.ckpt\n",
      "Restoring states from the checkpoint path at /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_3/epoch=0000-validation.loss=0.574.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_3/epoch=0000-validation.loss=0.574.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8ba57f5359481cb2673c4c3c84725c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/acc            0.7846002578735352\n",
      "        test/loss           0.5775994062423706\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "gpus = int(torch.cuda.is_available())  # use GPUs if they're available\n",
    "\n",
    "%run training/run_experiment.py --model_class CNN --data_class EMNIST --gpus {gpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training/logs/lightning_logs/version_3/epoch=0000-validation.loss=0.574.ckpt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use a sequence of bash commands to get the latest checkpoint's filename\n",
    "#  by hand, you can just copy and paste it\n",
    "\n",
    "list_all_log_files = \"find training/logs/lightning_logs\"  # find avoids issues with \\n in filenames\n",
    "filter_to_ckpts = \"grep \\.ckpt$\"  # regex match on end of line\n",
    "sort_version_descending = \"sort -Vr\"  # uses \"version\" sorting (-V) and reverses (-r)\n",
    "take_first = \"head -n 1\"  # the first n elements, n=1\n",
    "\n",
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "latest_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training.util\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "# if you change around model/data args in the command above, add them here\n",
    "#  tip: define the arguments as variables, like we've done for gpus\n",
    "#       and then add those variables to this dict so you don't need to\n",
    "#       remember to update/copy+paste\n",
    "\n",
    "args = Namespace(**{\n",
    "    \"model_class\": \"CNN\",\n",
    "    \"data_class\": \"EMNIST\"})\n",
    "\n",
    "\n",
    "_, cnn = training.util.setup_data_and_model_from_args(args)\n",
    "\n",
    "reloaded_model = text_recognizer.lit_models.BaseLitModel.load_from_checkpoint(\n",
    "   latest_ckpt, args=args, model=cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: l\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABU0lEQVR4nMXWsarCMBQG4NOk4GRrhOImOhQK0g71AXQvgu/hG/g8fYXi7CoOYiluXewgOFR0qjQnd+gD9NzLgfvPJ19OQkgC8L8ZDAa+74/HYzZRCLHZbG63236/7y+mo3EcB0Gw3W6llDxoF2PM4XDQWvOgSqkwDAEAEXuLqeh0Oo3jGAA+nw8bul6vJ5OJ1vp4PBKH9MS27fP5jIiPx8PzvN56aqfD4RAA0jR9Pp9saBfKhlJRpZRSij43CR2NRq7rAm+njuMIIdq2zbKMDU2SREppjKGcfBIqhIiiCADu93tVVWzoYrFAxMvl0jQND9q5iJjnOdvyu/NE31BSfN//fr9lWS6XS+KQ/k4dx7Esq2ma9/vNhq5WKyllURSv14sN7a6S6/Va1zUbCgBa6yzL2rZlQxHRGEPkqJnP56fTabfb9T6iv4hlWbPZjPMP8bf8AOTfhaa0N+wlAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x7F445D68ED60>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(xs) - 1)\n",
    "outs = reloaded_model(xs[idx:idx+1])\n",
    "\n",
    "print(\"output:\", emnist.mapping[torch.argmax(outs)])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type      | Params\n",
      "---------------------------------------------\n",
      "0 | model          | CNN       | 1.7 M \n",
      "1 | model.conv1    | ConvBlock | 640   \n",
      "2 | model.conv2    | ConvBlock | 36.9 K\n",
      "3 | model.dropout  | Dropout   | 0     \n",
      "4 | model.max_pool | MaxPool2d | 0     \n",
      "5 | model.fc1      | Linear    | 1.6 M \n",
      "6 | model.fc2      | Linear    | 10.7 K\n",
      "7 | train_acc      | Accuracy  | 0     \n",
      "8 | val_acc        | Accuracy  | 0     \n",
      "9 | test_acc       | Accuracy  | 0     \n",
      "---------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.616     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model State Dict Disk Size: 6.62 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e03aaddf5b4db4b7a674e79d57a8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4724b27510943559f4f8f181b73f032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e67632ab7b442d87d41ec53a603b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_4/epoch=0000-validation.loss=0.532.ckpt\n",
      "Restoring states from the checkpoint path at /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_4/epoch=0000-validation.loss=0.532.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_4/epoch=0000-validation.loss=0.532.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d2b28363634c3b89cacf8f985d479c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/acc            0.7939727306365967\n",
      "        test/loss           0.5348023176193237\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "\n",
    "\n",
    "# and we can change the training hyperparameters, like batch size\n",
    "%run training/run_experiment.py --model_class CNN --data_class EMNIST --gpus {gpus} \\\n",
    "  --batch_size 64 --load_checkpoint {latest_ckpt}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating lines of text from handwritten characters: `EMNISTLines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate text sentences using the Brown corpus.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text_recognizer.data.sentence_generator import SentenceGenerator\n",
    "\n",
    "sentence_generator = SentenceGenerator()\n",
    "\n",
    "SentenceGenerator.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase\n",
      "Within\n",
      "either\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(*[sentence_generator.generate(max_length=16) for _ in range(4)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EMNIST Lines dataset: synthetic handwriting lines dataset made from EMNIST characters.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist_lines = text_recognizer.data.EMNISTLines()  # configure\n",
    "emnist_lines.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNISTLinesDataset generating data for train...\n",
      "EMNISTLinesDataset generating data for val...\n",
      "EMNISTLinesDataset generating data for test...\n",
      "EMNISTLinesDataset loading data from HDF5...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EMNIST Lines Dataset\n",
       "Min overlap: 0\n",
       "Max overlap: 0.33\n",
       "Num classes: 83\n",
       "Dims: (1, 28, 896)\n",
       "Output dims: (32, 1)\n",
       "Train/val/test sizes: 10000, 2000, 2000\n",
       "Batch x stats: (torch.Size([128, 1, 28, 896]), torch.float32, 0.0, 0.07364349067211151, 0.23208004236221313, 1.0)\n",
       "Batch y stats: (torch.Size([128, 32]), torch.int64, 3, 66)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnist_lines.prepare_data()  # download, save to disk\n",
    "emnist_lines.setup()  # create torch.utils.data.Datasets, do train/val split\n",
    "emnist_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 896]), torch.Size([128, 32]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_xs, line_ys = next(iter(emnist_lines.val_dataloader()))\n",
    "line_xs.shape, line_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f-e-e-t- -H-e- -h-o-v-e-r-e-d- -o-v-e-r- -h-e-r-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=896x28 at 0x7F43E26C0EB0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_line_labels(labels):\n",
    "    return [emnist_lines.mapping[label] for label in labels]\n",
    "\n",
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "print(\"-\".join(read_line_labels(line_ys[idx])))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying CNNs to handwritten text: `LineCNNSimple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LineCNNSimple(\n",
       "  (cnn): CNN(\n",
       "    (conv1): ConvBlock(\n",
       "      (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (conv2): ConvBlock(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (fc1): Linear(in_features=12544, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=83, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_cnn = text_recognizer.models.LineCNNSimple(emnist_lines.config())\n",
    "line_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mline_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Apply the LineCNN to an input image and return logits.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        x\u001b[0m\n",
      "\u001b[0;34m            (B, C, H, W) input image with H equal to IMAGE_SIZE\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        torch.Tensor\u001b[0m\n",
      "\u001b[0;34m            (B, C, S) logits, where S is the length of the sequence and C is the number of classes\u001b[0m\n",
      "\u001b[0;34m            S can be computed from W and CHAR_WIDTH\u001b[0m\n",
      "\u001b[0;34m            C is self.num_classes\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m  \u001b[0;31m# Make sure we can use our CNN class\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Compute number of windows\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# NOTE: type_as properly sets device\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstart_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mend_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_w\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWW\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_w\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_w\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# -> (B, C, H, self.WW)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit_output_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# S might not match ground truth, so let's only take enough activations as are expected\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/git/fsdl-text-recognizer-2022-labs/text_recognizer/models/line_cnn_simple.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "line_cnn.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H-K-K-K-k-k-H-H-K-K-H-N-H-H-N-k-s-H-q-N-L-N-q-q-H-N-N-N-N-q-H-H\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=896x28 at 0x7F43E26C0CA0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "outs, = line_cnn(line_xs[idx:idx+1])\n",
    "preds = torch.argmax(outs, 0)\n",
    "\n",
    "print(\"-\".join(read_line_labels(preds)))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNISTLinesDataset loading data from HDF5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | model     | LineCNNSimple | 1.7 M \n",
      "1 | model.cnn | CNN           | 1.7 M \n",
      "2 | train_acc | Accuracy      | 0     \n",
      "3 | val_acc   | Accuracy      | 0     \n",
      "4 | test_acc  | Accuracy      | 0     \n",
      "--------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.616     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model State Dict Disk Size: 6.62 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d89b6c2964e41efb93b0342986b640b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbab367f3faf45be89538e8ec8ae2776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe1da74d72441fc8e7555effe9180e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a920870ec0440ab5f160e53599bda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_5/epoch=0001-validation.loss=1.372.ckpt\n",
      "Restoring states from the checkpoint path at /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_5/epoch=0001-validation.loss=1.372.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/amazingguni/git/fsdl-text-recognizer-2022-labs/training/logs/lightning_logs/version_5/epoch=0001-validation.loss=1.372.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNISTLinesDataset loading data from HDF5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afbfcd04b754e4eabf879863a9e8f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test/acc            0.6771718859672546\n",
      "        test/loss           1.4252547025680542\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "%run training/run_experiment.py --model_class LineCNNSimple --data_class EMNISTLines \\\n",
    "  --batch_size 32 --gpus {gpus} --max_epochs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/logs/lightning_logs/version_5/epoch=0001-validation.loss=1.372.ckpt\n"
     ]
    }
   ],
   "source": [
    "# if you change around model/data args in the command above, add them here\n",
    "#  tip: define the arguments as variables, like we've done for gpus\n",
    "#       and then add those variables to this dict so you don't need to\n",
    "#       remember to update/copy+paste\n",
    "\n",
    "args = Namespace(**{\n",
    "    \"model_class\": \"LineCNNSimple\",\n",
    "    \"data_class\": \"EMNISTLines\"})\n",
    "\n",
    "\n",
    "_, line_cnn = training.util.setup_data_and_model_from_args(args)\n",
    "\n",
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "print(latest_ckpt)\n",
    "\n",
    "reloaded_lines_model = text_recognizer.lit_models.BaseLitModel.load_from_checkpoint(\n",
    "   latest_ckpt, args=args, model=line_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i-s- -m-o-r-e- -e-l-e- -l-i-i-e-o-f-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=896x28 at 0x7F43E019F130>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "outs, = reloaded_lines_model(line_xs[idx:idx+1])\n",
    "preds = torch.argmax(outs, 0)\n",
    "\n",
    "print(\"-\".join(read_line_labels(preds)))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5359)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_token = emnist_lines.emnist.inverse_mapping[\"<P>\"]\n",
    "torch.sum(line_ys == padding_token) / line_ys.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsdl-text-recognizer-2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd45800261f6757616163cab2ac15de492f098dcc7245a9ac63e943c300526a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
