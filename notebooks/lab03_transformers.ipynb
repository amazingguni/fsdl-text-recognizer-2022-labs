{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: Transformers and Paragraphs\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "- The fundamental reasons why the Transformer is such\n",
    "a powerful and popular architecure\n",
    "- Core intuitions for the behavior of Transformer architectures\n",
    "- How to use a convolutional encoder and a Transformer decoder to recognize\n",
    "entire paragraphs of text\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "/home/amazingguni/git/fsdl-text-recognizer-2022-labs/notebooks\n",
      "\u001b[0m\u001b[01;34m__pycache__\u001b[0m/  lab01_pytorch.ipynb     lab02b_cnn.ipynb\n",
      "bootstrap.py  lab02a_lightning.ipynb  lab03_transformers.ipynb\n"
     ]
    }
   ],
   "source": [
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    # bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "base_url = \"https://fsdl-public-assets.s3.us-west-2.amazonaws.com\"\n",
    "\n",
    "display.Image(url=base_url + \"/aiayn-figure-1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.models import ResnetTransformer\n",
    "\n",
    "\n",
    "ResnetTransformer.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models import TransformerLitModel\n",
    "\n",
    "TransformerLitModel.training_step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformerLitModel.teacher_forward??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition #1: Transformers are highly residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(url=base_url + \"/transformer-residual-view.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition #2 Transformer heads learn low rank transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Latex(r\"$\\text{softmax}(Q \\cdot K^T) \\cdot V$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "low_rank = torch.randn(100, 1) @ torch.randn(1, 100)\n",
    "full_rank = torch.randn(100, 100)\n",
    "plt.figure(); plt.title(\"rank 1/100 matrix\"); plt.imshow(low_rank, cmap=\"Greys\"); plt.axis(\"off\")\n",
    "plt.figure(); plt.title(\"rank 100/100 matrix\");  plt.imshow(full_rank, cmap=\"Greys\"); plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuality and low rank together make Transformers less like a sequence model and more like a computer (that we can take gradients through)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(url=base_url + \"/transformer-layer-residual.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(url=base_url + \"/residual-stream-read-write.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(url=base_url + \"/residual-token-to-token.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation detail: Transformers are position-insensitive by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.models import transformer_util\n",
    "\n",
    "\n",
    "attention_mask = transformer_util.generate_square_subsequent_mask(100)\n",
    "\n",
    "ax = plt.matshow(torch.exp(attention_mask.T));  cb = plt.colorbar(ticks=[0, 1], fraction=0.05)\n",
    "plt.ylabel(\"Can the embedding at this index\"); plt.xlabel(\"attend to embeddings at this index?\")\n",
    "print(attention_mask[:10, :10].T); cb.set_ticklabels([False, True]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PositionalEncoder = transformer_util.PositionalEncoding(d_model=50, dropout=0.0, max_len=200)\n",
    "\n",
    "pe = PositionalEncoder.pe.squeeze().T[:, :]  # placing sequence dimension along the \"x-axis\"\n",
    "\n",
    "ax = plt.matshow(pe); plt.colorbar(ticks=[-1, 0, 1], fraction=0.05)\n",
    "plt.xlabel(\"sequence index\"); plt.ylabel(\"embedding dimension\"); plt.title(\"Positional Encoding\", y=1.1)\n",
    "print(pe[:4, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_embeddings = torch.randn_like(pe) * 0.5\n",
    "\n",
    "ax = plt.matshow(fake_embeddings); plt.colorbar(ticks=torch.arange(-2, 3), fraction=0.05)\n",
    "plt.xlabel(\"sequence index\"); plt.ylabel(\"embedding dimension\"); plt.title(\"Embeddings Without Positional Encoding\", y=1.1)\n",
    "\n",
    "fake_embeddings_with_pe = fake_embeddings + pe\n",
    "\n",
    "plt.matshow(fake_embeddings_with_pe); plt.colorbar(ticks=torch.arange(-2, 3), fraction=0.05)\n",
    "plt.xlabel(\"sequence index\"); plt.ylabel(\"embedding dimension\"); plt.title(\"Embeddings With Positional Encoding\", y=1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Transformers to read paragraphs of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_recognizer.data\n",
    "\n",
    "\n",
    "emnist_lines = text_recognizer.data.EMNISTLines()\n",
    "line_cnn = text_recognizer.models.LineCNNSimple(emnist_lines.config())\n",
    "\n",
    "# for sliding, see the for loop over range(S)\n",
    "line_cnn.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_paragraphs = text_recognizer.data.IAMParagraphs()\n",
    "\n",
    "iam_paragraphs.prepare_data()\n",
    "iam_paragraphs.setup()\n",
    "xs, ys = next(iter(iam_paragraphs.val_dataloader()))\n",
    "\n",
    "iam_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "\n",
    "def show(y):\n",
    "    y = y.detach().cpu()  # bring back from accelerator if it's being used\n",
    "    return \"\".join(np.array(iam_paragraphs.mapping)[y]).replace(\"<P>\", \"\")\n",
    "\n",
    "idx = random.randint(0, len(xs))\n",
    "\n",
    "print(show(ys[idx]))\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_recognizer.models\n",
    "\n",
    "\n",
    "rnt = text_recognizer.models.ResnetTransformer(data_config=iam_paragraphs.config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "rnt.to(device); xs = xs.to(device); ys = ys.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_embedding, = rnt.resnet(xs[idx:idx+1].repeat(1, 3, 1, 1))\n",
    " # resnet is designed for RGB images, so we replicate the input across channels 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_idx = random.randint(0, len(resnet_embedding))  # re-execute to view a different channel\n",
    "plt.matshow(resnet_embedding[resnet_idx].detach().cpu(), cmap=\"Greys_r\");\n",
    "plt.axis(\"off\"); plt.colorbar(fraction=0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, = rnt(xs[idx:idx+1])  # can take up to two minutes on a CPU. Transformers ❤️ GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show(preds.cpu()))\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_recognizer.lit_models\n",
    "\n",
    "lit_rnt = text_recognizer.lit_models.TransformerLitModel(rnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_outs, = lit_rnt.teacher_forward(xs[idx:idx+1], ys[idx:idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_preds = torch.argmax(forcing_outs, dim=0)\n",
    "\n",
    "print(show(forcing_preds.cpu()))\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the `ResNetTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gpus = int(torch.cuda.is_available())\n",
    "\n",
    "if gpus:\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"watch out! working with this model on a typical CPU is not feasible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# above %%magic times the cell, useful as a poor man's profiler\n",
    "\n",
    "%run training/run_experiment.py --data_class IAMParagraphs --model_class ResnetTransformer --loss transformer \\\n",
    "  --gpus={gpus} --batch_size 4 --precision 16 \\\n",
    "  --limit_train_batches 10 --limit_test_batches 1 --limit_val_batches 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsdl-text-recognizer-2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd45800261f6757616163cab2ac15de492f098dcc7245a9ac63e943c300526a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
